<!DOCTYPE html>
<html>
<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="google-site-verification" content="[object Object]" />
  <meta name="referrer" content="unsafe-url">
  
  <title>GPU性能比较</title>
  <meta name="author" content="Shuang Liang">
  <meta name="description" content="travel, life, code">
  
  
  <meta property="og:title" content="GPU性能比较"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:site_name" content="liangshuang&#39;s blog"/>
  <link href="/apple-touch-icon-precomposed.png" sizes="180x180" rel="apple-touch-icon-precomposed">
  <link rel="alternate" href="/atom.xml" title="liangshuang&#39;s blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/m.min.css">
  <link rel="icon" type="image/x-icon" href="/favicon.ico">
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

<!-- hexo injector head_end start -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4S0J2DBYMW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4S0J2DBYMW');
</script>
  <!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <a id="top"></a>
  <div id="main">
    <div class="main-ctnr">
      <div class="behind">
  <a href="/" class="back black-color">
    <svg class="i-close" viewBox="0 0 32 32" width="22" height="22" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3">
        <path d="M2 30 L30 2 M30 30 L2 2"></path>
    </svg>
  </a>
  
    <div class="description">
      &nbsp;supercool
    </div>
    
</div>


  <article class="standard post">
    <div class="title">
      
  
    <h1 class="page-title center">
        GPU性能比较
    </h1>
  


    </div>
    <div class="meta center">
      <time datetime="2025-04-09T04:13:15.000Z" itemprop="datePublished">
  <svg class="i-calendar" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
    <path d="M2 6 L2 30 30 30 30 6 Z M2 15 L30 15 M7 3 L7 9 M13 3 L13 9 M19 3 L19 9 M25 3 L25 9"></path>
  </svg>
  &nbsp;
  2025-04-09
</time>


    
    &nbsp;
    <svg class="i-tag" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <circle cx="24" cy="8" r="2"></circle>
      <path d="M2 18 L18 2 30 2 30 14 14 30 Z"></path>
    </svg>
    &nbsp;
    <a href="/categories/人工智能杂谈/">人工智能杂谈</a>





    </div>
    <hr>
    
    <div class="picture-container">
      
    </div>
    <h1 id="GPU-性能比较"><a href="#GPU-性能比较" class="headerlink" title="GPU 性能比较"></a>GPU 性能比较</h1><p>A100&#x2F;A800&#x2F;H100&#x2F;H800&#x2F;910B&#x2F;H200&#x2F;H20&#x2F;L20&#x2F;4090比较</p>
<div style="font-size: 14px;width:100%;overflow-x:auto;">

<table style="width:1800px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">A800 (PCIe&#x2F;SXM）</th>
<th align="center">A100 (PCIe&#x2F;SXM)</th>
<th align="center">H800 (PCIe&#x2F;SXM)</th>
<th align="center">H100 (PCIe&#x2F;SXM)</th>
<th align="center">H200 (PCIe&#x2F;SXM)</th>
<th align="center">H20</th>
<th align="center">L20 (PCIe)</th>
<th align="center">GeForce RTX 4090</th>
<th align="center">GeForce RTX 5090</th>
<th align="center">HGX B300</th>
<th align="center">GB300 NVL72</th>
<th align="center">HGX B200</th>
<th align="center">GB200 NVL72</th>
<th align="center">GB200 NVL4</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Year</td>
<td align="center">2022</td>
<td align="center">2022</td>
<td align="center">2023</td>
<td align="center">2024</td>
<td align="center">2024</td>
<td align="center">2023</td>
<td align="center">2023</td>
<td align="center">2022</td>
<td align="center">2024</td>
<td align="center">2025</td>
<td align="center">2025</td>
<td align="center">2025</td>
<td align="center">2025</td>
<td align="center">2025</td>
</tr>
<tr>
<td align="center">Manufacturing</td>
<td align="center">7nm</td>
<td align="center">5nm</td>
<td align="center">7nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
</tr>
<tr>
<td align="center">Architecture</td>
<td align="center">Ampere</td>
<td align="center">Ampere</td>
<td align="center">Hopper <br>(阉割)</td>
<td align="center">Hopper</td>
<td align="center">Hopper</td>
<td align="center">Hopper</td>
<td align="center">Ada Lovelace</td>
<td align="center">Ada Lovelace</td>
<td align="center">40MB</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
</tr>
<tr>
<td align="center">Max Power</td>
<td align="center">300&#x2F;400 W</td>
<td align="center">300&#x2F;400 W</td>
<td align="center">300-350&#x2F;700W</td>
<td align="center">350-400&#x2F;700 W</td>
<td align="center">600&#x2F;700W</td>
<td align="center">400W</td>
<td align="center">350W</td>
<td align="center">425W</td>
<td align="center">575W</td>
<td align="center">1100 W</td>
<td align="center">1400 W</td>
<td align="center">1000W</td>
<td align="center">1200 W</td>
<td align="center">1200 W</td>
</tr>
<tr>
<td align="center">CUDA</td>
<td align="center">8.0</td>
<td align="center">8.0</td>
<td align="center">9.0</td>
<td align="center">9.0</td>
<td align="center">9.0</td>
<td align="center">-</td>
<td align="center">8.9</td>
<td align="center">9.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
</tr>
<tr>
<td align="center">GPU Mem</td>
<td align="center">80G HBM2e</td>
<td align="center">80G HBM2e</td>
<td align="center">80G HBM3</td>
<td align="center">64G&#x2F;94G&#x2F;80G HBM3</td>
<td align="center">141GB HBM3e</td>
<td align="center">96G HBM3</td>
<td align="center">48G GDDR6</td>
<td align="center">24GB GDDR6X</td>
<td align="center">32 GB GDDR7</td>
<td align="center">270 GB HBM3e</td>
<td align="center">279 GB HBM3E</td>
<td align="center">Up to 192 GB HBM3e</td>
<td align="center">186 GB HBM3E</td>
<td align="center">186 GB HBM3E</td>
</tr>
<tr>
<td align="center">GPU Mem BW</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">1.9&#x2F;2TB&#x2F;s</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">3.9&#x2F;3.35 TB&#x2F;s</td>
<td align="center">4.8 TB&#x2F;s</td>
<td align="center">4TB&#x2F;s</td>
<td align="center">864GB&#x2F;s</td>
<td align="center">1008 GB&#x2F;s</td>
<td align="center">1792 GB&#x2F;s</td>
<td align="center">7.7 TB&#x2F;s</td>
<td align="center">8 TB&#x2F;s</td>
<td align="center">7.7 TB&#x2F;s</td>
<td align="center">8 TB&#x2F;s</td>
<td align="center">8 TB&#x2F;s</td>
</tr>
<tr>
<td align="center">GPU Interconnect <br> (one-to-one max bw)</td>
<td align="center">PCIe 4.0：64GB&#x2F;s <br> NVLink：400GB&#x2F;s</td>
<td align="center">PCIe Gen4 64GB&#x2F;s <br> NVLINK 600GB&#x2F;s</td>
<td align="center">NVLink: 400GB&#x2F;s <br> PCIe 5.0: 128GB&#x2F;s</td>
<td align="center">PCIe Gen5 128GB&#x2F;s<br>NVLINK 600&#x2F;900GB&#x2F;s</td>
<td align="center">PCIe Gen5 128GB&#x2F;s<br>NVLINK 900 GB&#x2F;s</td>
<td align="center">PCIe Gen5 128GB&#x2F;s<br>NVLINK 900GB&#x2F;s</td>
<td align="center">PCIe Gen4 64GB&#x2F;s</td>
<td align="center">PCIe 4.0 x16 (单向32 GB&#x2F;s, 双向64 GB&#x2F;s)</td>
<td align="center">-</td>
<td align="center">“NVLink 5：1.8 TB&#x2F;s</td>
<td align="center">PCIe Gen6：256 GB&#x2F;s”	“NVLink 5：1.8 TB&#x2F;s</td>
<td align="center">PCIe Gen6：256 GB&#x2F;s”	“NVLink 5：1.8 TB&#x2F;s</td>
<td align="center">PCIe Gen5：128 GB&#x2F;s”	“NVLink 5：1.8 TB&#x2F;s</td>
<td align="center">PCIe Gen5：128 GB&#x2F;s”	“NVLink 5：1.8 TB&#x2F;s</td>
</tr>
<tr>
<td align="center">GPU Interconnect <br> (one-to-many total bw)</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">1.9&#x2F;2TB&#x2F;s</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">3.9&#x2F;3.35 TB&#x2F;s</td>
<td align="center">4.8 TB&#x2F;s</td>
<td align="center">4TB&#x2F;s</td>
<td align="center">864GB&#x2F;s</td>
<td align="center">1TB&#x2F;s</td>
<td align="center">-</td>
<td align="center">“NVLink 5PCIe Gen6”</td>
<td align="center">-</td>
<td align="center">“NVLink 5 PCIe Gen6”</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">FP64  TFLOPS</td>
<td align="center">19.5</td>
<td align="center">19.5</td>
<td align="center">51 &#124; 67</td>
<td align="center">60 &#124; 67</td>
<td align="center">60 &#124; 67</td>
<td align="center">44</td>
<td align="center">59.8</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">1.2 teraFLOPS</td>
<td align="center">1.3 TFLOPS</td>
<td align="center">37 teraFLOPS</td>
<td align="center">40 TFLOPS</td>
<td align="center">40 TFLOPS</td>
</tr>
<tr>
<td align="center">FP32  TFLOPS</td>
<td align="center">19.5</td>
<td align="center">19.5</td>
<td align="center">51 &#124; 67</td>
<td align="center">60 &#124; 67</td>
<td align="center">60 &#124; 67</td>
<td align="center">44</td>
<td align="center">59.8</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">75 TFLOPS</td>
<td align="center">80 TFLOPS</td>
<td align="center">75 TFLOPS</td>
<td align="center">80 TFLOPS</td>
<td align="center">80 TFLOPS</td>
</tr>
<tr>
<td align="center">TF32  TFLOPS</td>
<td align="center">156 &#124; 312</td>
<td align="center">156 &#124; 312</td>
<td align="center">756  &#124; 989</td>
<td align="center">989 &#124; 853</td>
<td align="center">989 &#124; 853</td>
<td align="center">74</td>
<td align="center">59.8</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">1.1&#x2F;2.2 petaFLOPS</td>
<td align="center">2.5 PFLOPS</td>
<td align="center">1.1&#x2F;2.2 petaFLOPS</td>
<td align="center">2.5 PFLOPS</td>
<td align="center">2.5 PFLOPS</td>
</tr>
<tr>
<td align="center">BF16  TFLOPS</td>
<td align="center">156 &#124; 312</td>
<td align="center">312 &#124; 624</td>
<td align="center">1513  &#124; 1979</td>
<td align="center">1671  &#124; 1979</td>
<td align="center">1671  &#124; 1979</td>
<td align="center">148  &#124; 148</td>
<td align="center">119  &#124; 119</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">4.5 PLFOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">4.5 PFLOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">5 PFLOPS</td>
</tr>
<tr>
<td align="center">FP16  TFLOPS</td>
<td align="center">156 &#124; 312</td>
<td align="center">312 &#124; 624</td>
<td align="center">1513  &#124; 1979</td>
<td align="center">1671  &#124; 1979</td>
<td align="center">1671  &#124; 1979</td>
<td align="center">148  &#124; 14</td>
<td align="center">119  &#124; 119</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">2.2&#x2F;4.5 petaFLOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">2.2&#x2F;4.5 petaFLOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">5 PFLOPS</td>
</tr>
<tr>
<td align="center">FP8   TFLOPS</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">3026  &#124; 3958</td>
<td align="center">3341  &#124; 3958</td>
<td align="center">3341  &#124; 3958</td>
<td align="center">296  &#124; 296</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">4.5&#x2F;9 petaFLOPS</td>
<td align="center">10 PFLOPS</td>
<td align="center">4.5&#x2F;9 petaFLOPS</td>
<td align="center">10 PFLOPS</td>
<td align="center">10 PFLOPS</td>
</tr>
<tr>
<td align="center">INT8  TOPS</td>
<td align="center">NOT support</td>
<td align="center">624 &#124; 1248</td>
<td align="center">3026  &#124; 3958</td>
<td align="center">3341  &#124; 3958</td>
<td align="center">3341  &#124; 3958</td>
<td align="center">296  &#124; 296</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">0.15&#x2F; 0.30 petaOPS</td>
<td align="center">330 TOPS</td>
<td align="center">4.5&#x2F;9 petaOPS</td>
<td align="center">10 POPS</td>
<td align="center">10 POPS</td>
</tr>
<tr>
<td align="center">FP4 Tensor Core Dense&#x2F;Sparse</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">14 &#124; 18 petaFLOPS</td>
<td align="center">20 PFLOPS  &#124; 15 PFLOPS</td>
<td align="center">9&#x2F;18 petaFLOPS</td>
<td align="center">20 POPS</td>
<td align="center">20 PFLOPS</td>
</tr>
<tr>
<td align="center">L1 Cache</td>
<td align="center">192 KB</td>
<td align="center">192 KB</td>
<td align="center">256 KB</td>
<td align="center">256 KB</td>
<td align="center">256 KB</td>
<td align="center">-</td>
<td align="center">128 KB</td>
<td align="center">128 KB</td>
<td align="center">128 KB</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">L2 Cache</td>
<td align="center">40MB</td>
<td align="center">80MB</td>
<td align="center">50MB</td>
<td align="center">50MB</td>
<td align="center">50MB</td>
<td align="center">60MB</td>
<td align="center">96MB</td>
<td align="center">72MB</td>
<td align="center">96MB</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center"></tr></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
</table>
</div>

<div style="font-size: 14px;width:100%;overflow-x:auto;">

<table style="width:1800px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">HGX B300</th>
<th align="center">GB300 NVL72</th>
<th align="center">HGX B200</th>
<th align="center">GB200 NVL72</th>
<th align="center">GB200 NVL4</th>
<th align="center">GeForce RTX 5090</th>
<th align="center">H200 (PCIe&#x2F;SXM)</th>
<th align="center">H100 (PCIe&#x2F;SXM)</th>
<th align="center">H800 (PCIe&#x2F;SXM)</th>
<th align="center">H20</th>
<th align="center">L20 (PCIe)</th>
<th align="center">A800 (PCIe&#x2F;SXM)</th>
<th align="center">A100 (PCIe&#x2F;SXM)</th>
<th align="center">GeForce RTX 4090</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>Year</strong></td>
<td align="center"><strong>2025</strong></td>
<td align="center"><strong>2025</strong></td>
<td align="center"><strong>2025</strong></td>
<td align="center"><strong>2025</strong></td>
<td align="center"><strong>2025</strong></td>
<td align="center"><strong>2024</strong></td>
<td align="center"><strong>2024</strong></td>
<td align="center"><strong>2024</strong></td>
<td align="center"><strong>2023</strong></td>
<td align="center"><strong>2023</strong></td>
<td align="center"><strong>2023</strong></td>
<td align="center"><strong>2022</strong></td>
<td align="center"><strong>2022</strong></td>
<td align="center"><strong>2022</strong></td>
</tr>
<tr>
<td align="center">Manufacturing</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">7nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">7nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
</tr>
<tr>
<td align="center">Architecture</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Hopper</td>
<td align="center">Hopper</td>
<td align="center">Hopper (阉割)</td>
<td align="center">Hopper</td>
<td align="center">Ada Lovelace</td>
<td align="center">Ampere</td>
<td align="center">Ampere</td>
<td align="center">Ada Lovelace</td>
</tr>
<tr>
<td align="center">Max Power</td>
<td align="center">1100 W</td>
<td align="center">1400 W</td>
<td align="center">1000W</td>
<td align="center">1200 W</td>
<td align="center">1200 W</td>
<td align="center">575W</td>
<td align="center">600&#x2F;700W</td>
<td align="center">350-400&#x2F;700 W</td>
<td align="center">300-350&#x2F;700W</td>
<td align="center">400W</td>
<td align="center">350W</td>
<td align="center">300&#x2F;400 W</td>
<td align="center">300&#x2F;400 W</td>
<td align="center">425W</td>
</tr>
<tr>
<td align="center">CUDA</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">9.0</td>
<td align="center">9.0</td>
<td align="center">9.0</td>
<td align="center">-</td>
<td align="center">8.9</td>
<td align="center">8.0</td>
<td align="center">8.0</td>
<td align="center">9.0</td>
</tr>
<tr>
<td align="center">GPU Mem</td>
<td align="center">270 GB HBM3e</td>
<td align="center">279 GB HBM3E</td>
<td align="center">Up to 192 GB HBM3e</td>
<td align="center">186 GB HBM3E</td>
<td align="center">186 GB HBM3E</td>
<td align="center">32 GB GDDR7</td>
<td align="center">141GB HBM3e</td>
<td align="center">64G&#x2F;94G&#x2F;80G HBM3</td>
<td align="center">80G HBM3</td>
<td align="center">96G HBM3</td>
<td align="center">48G GDDR6</td>
<td align="center">80G HBM2e</td>
<td align="center">80G HBM2e</td>
<td align="center">24GB GDDR6X</td>
</tr>
<tr>
<td align="center">GPU Mem BW</td>
<td align="center">7.7 TB&#x2F;s</td>
<td align="center">8 TB&#x2F;s</td>
<td align="center">7.7 TB&#x2F;s</td>
<td align="center">8 TB&#x2F;s</td>
<td align="center">8 TB&#x2F;s</td>
<td align="center">1792 GB&#x2F;s</td>
<td align="center">4.8 TB&#x2F;s</td>
<td align="center">3.9&#x2F;3.35 TB&#x2F;s</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">4TB&#x2F;s</td>
<td align="center">864GB&#x2F;s</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">1.9&#x2F;2TB&#x2F;s</td>
<td align="center">1008 GB&#x2F;s</td>
</tr>
<tr>
<td align="center">GPU Interconnect <br> (one-to-one max bw)</td>
<td align="center">NVLink 5：1.8 TB&#x2F;s <br> PCIe Gen6：256 GB&#x2F;s</td>
<td align="center">NVLink 5：1.8 TB&#x2F;s <br> PCIe Gen6：256 GB&#x2F;s</td>
<td align="center">NVLink 5：1.8 TB&#x2F;s <br> PCIe Gen5：128 GB&#x2F;s</td>
<td align="center">NVLink 5：1.8 TB&#x2F;s <br> PCIe Gen5：128 GB&#x2F;s</td>
<td align="center">NVLink 5：1.8 TB&#x2F;s <br> PCIe Gen5：128 GB&#x2F;s</td>
<td align="center">-</td>
<td align="center">PCIe Gen5 128GB&#x2F;s <br> NVLINK 900 GB&#x2F;s</td>
<td align="center">PCIe Gen5 128GB&#x2F;s <br> NVLINK 600&#x2F;900GB&#x2F;s</td>
<td align="center">NVLink: 400GB&#x2F;s <br> PCIe 5.0: 128GB&#x2F;s</td>
<td align="center">PCIe Gen5 128GB&#x2F;s <br> NVLINK 900GB&#x2F;s</td>
<td align="center">PCIe Gen4 64GB&#x2F;s</td>
<td align="center">PCIe 4.0：64GB&#x2F;s <br> NVLink：400GB&#x2F;s</td>
<td align="center">PCIe Gen4 64GB&#x2F;s <br> NVLINK 600GB&#x2F;s</td>
<td align="center">PCIe 4.0 x16 (单向32 GB&#x2F;s, 双向64 GB&#x2F;s)</td>
</tr>
<tr>
<td align="center">GPU Interconnect <br> (one-to-many total bw)</td>
<td align="center">NVLink 5 PCIe Gen6</td>
<td align="center">-</td>
<td align="center">NVLink 5 PCIe Gen6</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">4.8 TB&#x2F;s</td>
<td align="center">3.9&#x2F;3.35 TB&#x2F;s</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">4TB&#x2F;s</td>
<td align="center">864GB&#x2F;s</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">1.9&#x2F;2TB&#x2F;s</td>
<td align="center">1TB&#x2F;s</td>
</tr>
<tr>
<td align="center">FP64 TFLOPS</td>
<td align="center">1.2 teraFLOPS</td>
<td align="center">1.3 TFLOPS</td>
<td align="center">37 teraFLOPS</td>
<td align="center">40 TFLOPS</td>
<td align="center">40 TFLOPS</td>
<td align="center">-</td>
<td align="center">60 | 67</td>
<td align="center">60 | 67</td>
<td align="center">51 | 67</td>
<td align="center">44</td>
<td align="center">59.8</td>
<td align="center">19.5</td>
<td align="center">19.5</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">FP32 TFLOPS</td>
<td align="center">75 TFLOPS</td>
<td align="center">80 TFLOPS</td>
<td align="center">75 TFLOPS</td>
<td align="center">80 TFLOPS</td>
<td align="center">80 TFLOPS</td>
<td align="center">-</td>
<td align="center">60 | 67</td>
<td align="center">60 | 67</td>
<td align="center">51 | 67</td>
<td align="center">44</td>
<td align="center">59.8</td>
<td align="center">19.5</td>
<td align="center">19.5</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">TF32 TFLOPS</td>
<td align="center">1.1&#x2F;2.2 petaFLOPS</td>
<td align="center">2.5 PFLOPS</td>
<td align="center">1.1&#x2F;2.2 petaFLOPS</td>
<td align="center">2.5 PFLOPS</td>
<td align="center">2.5 PFLOPS</td>
<td align="center">-</td>
<td align="center">989 | 853</td>
<td align="center">989 | 853</td>
<td align="center">756 | 989</td>
<td align="center">74</td>
<td align="center">59.8</td>
<td align="center">156 | 312</td>
<td align="center">156 | 312</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">BF16 TFLOPS</td>
<td align="center">4.5 PLFOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">4.5 PFLOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">-</td>
<td align="center">1671 | 1979</td>
<td align="center">1671 | 1979</td>
<td align="center">1513 | 1979</td>
<td align="center">148 | 148</td>
<td align="center">119 | 119</td>
<td align="center">156 | 312</td>
<td align="center">312 | 624</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">FP16 TFLOPS</td>
<td align="center">2.2&#x2F;4.5 petaFLOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">2.2&#x2F;4.5 petaFLOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">-</td>
<td align="center">1671 | 1979</td>
<td align="center">1671 | 1979</td>
<td align="center">1513 | 1979</td>
<td align="center">148 | 14</td>
<td align="center">119 | 119</td>
<td align="center">156 | 312</td>
<td align="center">312 | 624</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">FP8 TFLOPS</td>
<td align="center">4.5&#x2F;9 petaFLOPS</td>
<td align="center">10 PFLOPS</td>
<td align="center">4.5&#x2F;9 petaFLOPS</td>
<td align="center">10 PFLOPS</td>
<td align="center">10 PFLOPS</td>
<td align="center">-</td>
<td align="center">3341 | 3958</td>
<td align="center">3341 | 3958</td>
<td align="center">3026 | 3958</td>
<td align="center">296 | 296</td>
<td align="center">-</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">INT8 TOPS</td>
<td align="center">0.15&#x2F; 0.30 petaOPS</td>
<td align="center">330 TOPS</td>
<td align="center">4.5&#x2F;9 petaOPS</td>
<td align="center">10 POPS</td>
<td align="center">10 POPS</td>
<td align="center">-</td>
<td align="center">3341 | 3958</td>
<td align="center">3341 | 3958</td>
<td align="center">3026 | 3958</td>
<td align="center">296 | 296</td>
<td align="center">-</td>
<td align="center">NOT support</td>
<td align="center">624 | 1248</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">FP4 Tensor Core Dense&#x2F;Sparse</td>
<td align="center">14 | 18 petaFLOPS</td>
<td align="center">20 PFLOPS | 15 PFLOPS</td>
<td align="center">9&#x2F;18 petaFLOPS</td>
<td align="center">20 POPS</td>
<td align="center">20 PFLOPS</td>
<td align="center">-</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">-</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">L1 Cache</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">128 KB</td>
<td align="center">256 KB</td>
<td align="center">256 KB</td>
<td align="center">256 KB</td>
<td align="center">-</td>
<td align="center">128 KB</td>
<td align="center">192 KB</td>
<td align="center">192 KB</td>
<td align="center">128 KB</td>
</tr>
<tr>
<td align="center">L2 Cache</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">96MB</td>
<td align="center">50MB</td>
<td align="center">50MB</td>
<td align="center">50MB</td>
<td align="center">60MB</td>
<td align="center">96MB</td>
<td align="center">40MB</td>
<td align="center">80MB</td>
<td align="center">72MB</td>
</tr>
<tr>
<td align="center"></tr></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
</table>

</div>


<h2 id="📅-修正后的显卡规格对比-2025年到2022年"><a href="#📅-修正后的显卡规格对比-2025年到2022年" class="headerlink" title="📅 修正后的显卡规格对比 (2025年到2022年)"></a>📅 修正后的显卡规格对比 (2025年到2022年)</h2><div style="font-size: 14px;width:100%;overflow-x:auto;">
<table style="width:1800px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">HGX B300</th>
<th align="center">GB300 NVL72</th>
<th align="center">HGX B200</th>
<th align="center">GB200 NVL72</th>
<th align="center">GB200 NVL4</th>
<th align="center">GeForce RTX 5090</th>
<th align="center">H200 (PCIe&#x2F;SXM)</th>
<th align="center">H100 (PCIe&#x2F;SXM)</th>
<th align="center">H800 (PCIe&#x2F;SXM)</th>
<th align="center">H20</th>
<th align="center">L20 (PCIe)</th>
<th align="center">A800 (PCIe&#x2F;SXM)</th>
<th align="center">A100 (PCIe&#x2F;SXM)</th>
<th align="center">GeForce RTX 4090</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>Year</strong></td>
<td align="center"><strong>2025</strong></td>
<td align="center"><strong>2025</strong></td>
<td align="center"><strong>2025</strong></td>
<td align="center"><strong>2025</strong></td>
<td align="center"><strong>2025</strong></td>
<td align="center"><strong>2024</strong></td>
<td align="center"><strong>2024</strong></td>
<td align="center"><strong>2024</strong></td>
<td align="center"><strong>2023</strong></td>
<td align="center"><strong>2023</strong></td>
<td align="center"><strong>2023</strong></td>
<td align="center"><strong>2022</strong></td>
<td align="center"><strong>2022</strong></td>
<td align="center"><strong>2022</strong></td>
</tr>
<tr>
<td align="center">Manufacturing</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">7nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
<td align="center">7nm</td>
<td align="center">5nm</td>
<td align="center">5nm</td>
</tr>
<tr>
<td align="center"><strong>Architecture</strong></td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center">Blackwell</td>
<td align="center"><strong>Blackwell</strong></td>
<td align="center">Hopper</td>
<td align="center">Hopper</td>
<td align="center">Hopper (阉割)</td>
<td align="center">Hopper</td>
<td align="center">Ada Lovelace</td>
<td align="center">Ampere</td>
<td align="center">Ampere</td>
<td align="center">Ada Lovelace</td>
</tr>
<tr>
<td align="center">Max Power</td>
<td align="center">1100 W</td>
<td align="center">1400 W</td>
<td align="center">1000W</td>
<td align="center">1200 W</td>
<td align="center">1200 W</td>
<td align="center">575W</td>
<td align="center">600&#x2F;700W</td>
<td align="center">350-400&#x2F;700 W</td>
<td align="center">300-350&#x2F;700W</td>
<td align="center">400W</td>
<td align="center">350W</td>
<td align="center">300&#x2F;400 W</td>
<td align="center">300&#x2F;400 W</td>
<td align="center">425W</td>
</tr>
<tr>
<td align="center">CUDA</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">12.0</td>
<td align="center">9.0</td>
<td align="center">9.0</td>
<td align="center">9.0</td>
<td align="center">-</td>
<td align="center">8.9</td>
<td align="center">8.0</td>
<td align="center">8.0</td>
<td align="center">9.0</td>
</tr>
<tr>
<td align="center">GPU Mem</td>
<td align="center">270 GB HBM3e</td>
<td align="center">279 GB HBM3E</td>
<td align="center">Up to 192 GB HBM3e</td>
<td align="center">186 GB HBM3E</td>
<td align="center">186 GB HBM3E</td>
<td align="center">32 GB GDDR7</td>
<td align="center">141GB HBM3e</td>
<td align="center">64G&#x2F;94G&#x2F;80G HBM3</td>
<td align="center">80G HBM3</td>
<td align="center">96G HBM3</td>
<td align="center">48G GDDR6</td>
<td align="center">80G HBM2e</td>
<td align="center">80G HBM2e</td>
<td align="center">24GB GDDR6X</td>
</tr>
<tr>
<td align="center">GPU Mem BW</td>
<td align="center">7.7 TB&#x2F;s</td>
<td align="center">8 TB&#x2F;s</td>
<td align="center">7.7 TB&#x2F;s</td>
<td align="center">8 TB&#x2F;s</td>
<td align="center">8 TB&#x2F;s</td>
<td align="center">1792 GB&#x2F;s</td>
<td align="center">4.8 TB&#x2F;s</td>
<td align="center">3.9&#x2F;3.35 TB&#x2F;s</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">4TB&#x2F;s</td>
<td align="center">864GB&#x2F;s</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">1.9&#x2F;2TB&#x2F;s</td>
<td align="center">1008 GB&#x2F;s</td>
</tr>
<tr>
<td align="center">GPU Interconnect <br> (one-to-one max bw)</td>
<td align="center">NVLink 5：1.8 TB&#x2F;s <br> PCIe Gen6：256 GB&#x2F;s</td>
<td align="center">NVLink 5：1.8 TB&#x2F;s <br> PCIe Gen6：256 GB&#x2F;s</td>
<td align="center">NVLink 5：1.8 TB&#x2F;s <br> PCIe Gen5：128 GB&#x2F;s</td>
<td align="center">NVLink 5：1.8 TB&#x2F;s <br> PCIe Gen5：128 GB&#x2F;s</td>
<td align="center">NVLink 5：1.8 TB&#x2F;s <br> PCIe Gen5：128 GB&#x2F;s</td>
<td align="center">-</td>
<td align="center">PCIe Gen5 128GB&#x2F;s <br> NVLINK 900 GB&#x2F;s</td>
<td align="center">PCIe Gen5 128GB&#x2F;s <br> NVLINK 600&#x2F;900GB&#x2F;s</td>
<td align="center">NVLink: 400GB&#x2F;s <br> PCIe 5.0: 128GB&#x2F;s</td>
<td align="center">PCIe Gen5 128GB&#x2F;s <br> NVLINK 900GB&#x2F;s</td>
<td align="center">PCIe Gen4 64GB&#x2F;s</td>
<td align="center">PCIe 4.0：64GB&#x2F;s <br> NVLink：400GB&#x2F;s</td>
<td align="center">PCIe Gen4 64GB&#x2F;s <br> NVLINK 600GB&#x2F;s</td>
<td align="center">PCIe 4.0 x16 (单向32 GB&#x2F;s, 双向64 GB&#x2F;s)</td>
</tr>
<tr>
<td align="center">GPU Interconnect <br> (one-to-many total bw)</td>
<td align="center">NVLink 5 PCIe Gen6</td>
<td align="center">-</td>
<td align="center">NVLink 5 PCIe Gen6</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">4.8 TB&#x2F;s</td>
<td align="center">3.9&#x2F;3.35 TB&#x2F;s</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">4TB&#x2F;s</td>
<td align="center">864GB&#x2F;s</td>
<td align="center">2TB&#x2F;s</td>
<td align="center">1.9&#x2F;2TB&#x2F;s</td>
<td align="center">1TB&#x2F;s</td>
</tr>
<tr>
<td align="center"><strong>FP64 TFLOPS</strong></td>
<td align="center">600 TFLOPS (系统)</td>
<td align="center">2880 TFLOPS (系统)</td>
<td align="center">37 TFLOPS</td>
<td align="center">40 TFLOPS</td>
<td align="center">40 TFLOPS</td>
<td align="center">-</td>
<td align="center">67 TFLOPS</td>
<td align="center">60 TFLOPS</td>
<td align="center">51 TFLOPS</td>
<td align="center">44 TFLOPS</td>
<td align="center">59.8 TFLOPS</td>
<td align="center">19.5 TFLOPS</td>
<td align="center">19.5 TFLOPS</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center"><strong>FP32 TFLOPS</strong></td>
<td align="center">600 TFLOPS</td>
<td align="center">5760 TFLOPS</td>
<td align="center">75 TFLOPS</td>
<td align="center">80 TFLOPS</td>
<td align="center">80 TFLOPS</td>
<td align="center">-</td>
<td align="center">67 TFLOPS</td>
<td align="center">60 TFLOPS</td>
<td align="center">51 TFLOPS</td>
<td align="center">44 TFLOPS</td>
<td align="center">59.8 TFLOPS</td>
<td align="center">19.5 TFLOPS</td>
<td align="center">19.5 TFLOPS</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center"><strong>TF32 TFLOPS (稀疏)</strong></td>
<td align="center">36 PFLOPS</td>
<td align="center">60 PFLOPS</td>
<td align="center">18 PFLOPS</td>
<td align="center">2.5 PFLOPS</td>
<td align="center">2.5 PFLOPS</td>
<td align="center">-</td>
<td align="center">989 TFLOPS</td>
<td align="center">853 TFLOPS</td>
<td align="center">989 TFLOPS</td>
<td align="center">74 TFLOPS</td>
<td align="center">59.8 TFLOPS</td>
<td align="center">312 TFLOPS</td>
<td align="center">312 TFLOPS</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center"><strong>BF16 TFLOPS (稀疏)</strong></td>
<td align="center">72 PFLOPS</td>
<td align="center">120 PFLOPS</td>
<td align="center">36 PFLOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">5 PFLOPS</td>
<td align="center">-</td>
<td align="center">1979 TFLOPS</td>
<td align="center">1979 TFLOPS</td>
<td align="center">1979 TFLOPS</td>
<td align="center">148 TFLOPS</td>
<td align="center">119 TFLOPS</td>
<td align="center">312 TFLOPS</td>
<td align="center">624 TFLOPS</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center"><strong>FP8 TFLOPS (稀疏)</strong></td>
<td align="center">144 PFLOPS</td>
<td align="center">240 PFLOPS</td>
<td align="center">72 PFLOPS</td>
<td align="center">10 PFLOPS</td>
<td align="center">10 PFLOPS</td>
<td align="center">-</td>
<td align="center">3958 TFLOPS</td>
<td align="center">3958 TFLOPS</td>
<td align="center">3958 TFLOPS</td>
<td align="center">296 TFLOPS</td>
<td align="center">-</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center"><strong>FP4 PFLOPS (稀疏)</strong></td>
<td align="center">144 PFLOPS</td>
<td align="center">1440 PFLOPS</td>
<td align="center">72 PFLOPS</td>
<td align="center">20 PFLOPS</td>
<td align="center">20 PFLOPS</td>
<td align="center">-</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">-</td>
<td align="center">NOT support</td>
<td align="center">NOT support</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">INT8 TOPS</td>
<td align="center">2 POPS</td>
<td align="center">-</td>
<td align="center">10 POPS</td>
<td align="center">10 POPS</td>
<td align="center">10 POPS</td>
<td align="center">-</td>
<td align="center">3958 TOPS</td>
<td align="center">3958 TOPS</td>
<td align="center">3958 TOPS</td>
<td align="center">296 TOPS</td>
<td align="center">-</td>
<td align="center">NOT support</td>
<td align="center">1248 TOPS</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">L1 Cache</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">128 KB</td>
<td align="center">256 KB</td>
<td align="center">256 KB</td>
<td align="center">256 KB</td>
<td align="center">-</td>
<td align="center">128 KB</td>
<td align="center">192 KB</td>
<td align="center">192 KB</td>
<td align="center">128 KB</td>
</tr>
<tr>
<td align="center">L2 Cache</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">96MB</td>
<td align="center">50MB</td>
<td align="center">50MB</td>
<td align="center">50MB</td>
<td align="center">60MB</td>
<td align="center">96MB</td>
<td align="center">40MB</td>
<td align="center">80MB</td>
<td align="center">72MB</td>
</tr>
<tr>
<td align="center"></tr></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
</table>
</div>

<p><strong>备注：</strong></p>
<ol>
<li><strong>RTX 5090 架构：</strong> 已从 “40MB” 更正为 <strong>Blackwell</strong>。</li>
<li><strong>Blackwell 性能：</strong> 对 <strong>B300&#x2F;GB200</strong> 系列的 <strong>TFLOPS&#x2F;PFLOPS&#x2F;POPS</strong> 单位进行了大幅修正，采用了官方公布的系统级性能数据（如 <code>GB200 NVL72</code> 的性能是 72 个 Blackwell GPU 的总和）。</li>
<li><strong>H100&#x2F;H200 性能：</strong> 修正了 H200 的 TF32&#x2F;BF16&#x2F;FP8 TFLOPS 数据，使其与官方规格保持一致。</li>
</ol>
<p>先看NVIDIA官网的数据表，比如A100的FP16算力是312 TFLOPS，但这是Tensor Core的稀疏性能。<br>实际常用的是稠密（Dense）算力，所以A100 80G NVLink的FP16稠密算力是156 TFLOPS单卡，乘以8卡就是1248 TFLOPS，换算成P就是1.25 PFLOPS。这里要注意NVLink对多卡效率的提升，但算力本身是线性叠加的。</p>
<p>$$1 Petaflops &#x3D; 1,000 Teraflops$$<br>Flops：是“每秒浮点运算次数”的缩写，是衡量计算机计算能力的基本单位。</p>
<p>Teraflops：代表“每秒一万亿次浮点运算”。<br>“Tera-” 这个前缀在公制系统中代表 10¹²，即 1,000,000,000,000（一万亿）。<br>所以:<br>$$1 Teraflops &#x3D; 10¹² Flops$$</p>
<p>Petaflops：代表“每秒一千万亿次浮点运算”。<br>“Peta-” 这个前缀代表 10¹⁵，即 1,000,000,000,000,000（一千万亿）。<br>所以;<br>$$1 Petaflops &#x3D; 10¹⁵ Flops$$</p>
<h1 id="深度学习中的精度格式一览"><a href="#深度学习中的精度格式一览" class="headerlink" title="深度学习中的精度格式一览"></a>深度学习中的精度格式一览</h1><p>不同的精度格式在位宽、内存占用、计算速度和数值精度方面各有特点。选择合适的格式是优化模型性能和效率的关键。<br>浮点数格式 (Floating Point)：</p>
<div style="font-size: 14px;width:100%;overflow-x:auto;">
<table style="width:1000px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="center">格式</th>
<th align="center">全称 (Bits)</th>
<th align="center">结构 (符号&#x2F;指数&#x2F;尾数)</th>
<th align="center">内存 (Bytes)</th>
<th align="left">深度学习中的角色</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>FP64</strong></td>
<td align="center">双精度浮点数 (64)</td>
<td align="center">1 &#x2F; 11 &#x2F; 52</td>
<td align="center">8</td>
<td align="left">科学计算&#x2F;测试基准：提供最高精度，主要用于科学计算或作为模型训练的精度黄金标准。</td>
</tr>
<tr>
<td align="center"><strong>FP32</strong></td>
<td align="center">单精度浮点数 (32)</td>
<td align="center">1 &#x2F; 8 &#x2F; 23</td>
<td align="center">4</td>
<td align="left">标准训练&#x2F;推理格式：传统上用于训练和推理的标准格式，提供足够的动态范围和动态范围。</td>
</tr>
<tr>
<td align="center"><strong>TF32</strong></td>
<td align="center">TensorFloat-32 (19)</td>
<td align="center">1 &#x2F; 8 &#x2F; 10</td>
<td align="center">4</td>
<td align="left">NVIDIA 加速训练格式：仅在 NVIDIA Tensor Core 上可用，它拥有 FP32 的动态范围（8位指数）和 FP16 的精度（10位尾数），加速训练同时保持接近 FP32 的效果。</td>
</tr>
<tr>
<td align="center"><strong>FP16</strong></td>
<td align="center">半精度浮点数 (16)</td>
<td align="center">1 &#x2F; 5 &#x2F; 10</td>
<td align="center">2</td>
<td align="left">通用混合精度格式：显著节省内存并加速计算。在混合精度训练和推理中广泛使用。</td>
</tr>
<tr>
<td align="center"><strong>BF16</strong></td>
<td align="center">Brain Floating Point (16)</td>
<td align="center">1 &#x2F; 8 &#x2F; 7</td>
<td align="center">2</td>
<td align="left">友好混合精度格式：与 FP16 共用 16 位，但具有与 FP32 相同的 8 位指数，因此动态范围更大，更不易溢出。在 Google TPU 和部分 NVIDIA 硬件上流行。</td>
</tr>
<tr>
<td align="center"><strong>FP8</strong></td>
<td align="center">8 位浮点数 (8)</td>
<td align="center">多种实体 (如 E5M2, E4M3)</td>
<td align="center">1</td>
<td align="left">超低精度推理：进一步降低内存和延迟，通常需要专用的硬件支持（如最新的 NVIDIA Tensor Core）。</td>
</tr>
<tr>
<td align="center"><strong>NVFP4</strong></td>
<td align="center">NVIDIA 4 位浮点 (4)</td>
<td align="center">NVIDIA 专有</td>
<td align="center">0.5</td>
<td align="left">极限压缩推理：专为 NVIDIA 平台设计，用于在极低带宽下优化 LLM 推理性能。</td>
</tr>
<tr>
<td align="center"></tr></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="left"></td>
</tr>
</tbody></table>
</table>
</div>

<p>整数格式 (Integer)：</p>
<div style="font-size: 14px;width:100%;overflow-x:auto;">
<table style="width:1000px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="center">格式</th>
<th align="center">位数 (Bits)</th>
<th align="center">内存 (Bytes)</th>
<th align="left">深度学习中的角色</th>
<th align="left">常用配置</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>INT8</strong></td>
<td align="center">8 位整数 (8)</td>
<td align="center">1</td>
<td align="left">通用量化推理：业界标准的量化格式，通过将 FP32&#x2F;FP16 数据映射到 8 位整数，实现推理速度和内存的显著优化，精度损失相对可控。</td>
<td align="left">W8A8 (权重和激活值都是 INT8)</td>
</tr>
<tr>
<td align="center"><strong>INT4</strong></td>
<td align="center">4 位整数 (4)</td>
<td align="center">0.5</td>
<td align="left">极限量化推理：进一步将模型大小减半，常用于大型 LLM 的部署（如 Llama, Mistral），以解决内存限制问题。</td>
<td align="left">W4A16 (权重 INT4，激活值 FP16&#x2F;BF16)</td>
</tr>
<tr>
<td align="center"></tr></td>
<td align="center"></td>
<td align="center"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
</table>
</div>

<p>总结</p>
<div style="font-size: 14px;width:100%;overflow-x:auto;">
<table style="width:1000px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="center">优化目标</th>
<th align="center">推荐格式</th>
<th align="left">优点</th>
<th align="left">缺点&#x2F;要求</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>最大精度</strong></td>
<td align="center">FP64, FP32</td>
<td align="left">最高的数值稳定性和动态范围。</td>
<td align="left">内存占用高，计算速度慢。</td>
</tr>
<tr>
<td align="center"><strong>加速训练</strong></td>
<td align="center">TF32, BF16</td>
<td align="left">在保持动态范围的同时加速训练。</td>
<td align="left">需要特定硬件（如 Tensor Core, TPU）。</td>
</tr>
<tr>
<td align="center"><strong>通用混合精度</strong></td>
<td align="center">FP16, BF16</td>
<td align="left">平衡了<strong>速度、内存和精度</strong>，是目前最主流的训练&#x2F;推理格式。</td>
<td align="left">FP16 动态范围较小，可能需要做操作。</td>
</tr>
<tr>
<td align="center"><strong>内存&#x2F;速度优化</strong></td>
<td align="center">INT8, FP8</td>
<td align="left">内存占用极低，计算吞吐量最高。</td>
<td align="left">精度损失风险增加，需要进行量化操作，可能需要专用硬件。</td>
</tr>
<tr>
<td align="center"></tr></td>
<td align="center"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
</table>
</div>

<h2 id="结构-符号-指数-尾数-是什么意思？"><a href="#结构-符号-指数-尾数-是什么意思？" class="headerlink" title="结构 (符号&#x2F;指数&#x2F;尾数) 是什么意思？"></a>结构 (符号&#x2F;指数&#x2F;尾数) 是什么意思？</h2><p>一个浮点数 $$V$$ 可以通过以下公式计算出来：</p>
<p>$$V &#x3D; (-1)^S \times (1 + M) \times 2^{(E - \text{Bias})}$$</p>
<p>$$S$$ : 符号位 (0 或 1)</p>
<p>$$M$$ : 尾数域的值</p>
<p>$$E$$ : 指数域的编码值</p>
<p>$$\text{Bias}$$ : 固定的偏移量（如 FP32 为 127，FP16 为 15）</p>
<p>这种结构决定了浮点数的两个关键特性：</p>
<ol>
<li><strong>动态范围（Exponent 决定）</strong>： 指数位越多，能表示的数值范围越大（从极小数到极大数），不易溢出。</li>
<li><strong>精度（Mantissa 决定）</strong>： 尾数位越多，能表示的有效数字越精确，舍入误差越小。</li>
</ol>
<p>在 LLM 量化中，我们就是通过减少这些位宽（特别是指数和尾数）来达到压缩内存的目的，但必须使用像 AWQ 这样的算法来最小化随之而来的精度损失。</p>
<p>📝 以 FP16 为例（半精度浮点数）<br>FP16 共有 16 位，结构划分如下：</p>
<ul>
<li><strong>优点</strong>： 相比 FP32 占用一半内存，在深度学习中能显著加速运算。</li>
<li><strong>权衡</strong>： 只有 5 位指数（动态范围较小）和 10 位尾数（精度较低），容易出现溢出（Overflow）或舍入误差。</li>
</ul>
<p>📝 以 FP32 为例（单精度浮点数）<br>FP32 共有 32 位，结构划分如下：</p>
<ul>
<li><strong>优点</strong>： 动态范围大（8 位指数），精度高（23 位尾数）。</li>
<li><strong>权衡</strong>： 内存占用较大（4 字节）。</li>
</ul>
<h3 id="低精度数据格式"><a href="#低精度数据格式" class="headerlink" title="低精度数据格式"></a>低精度数据格式</h3><p>好的，我们来详细解析一下您提到的这几种重要的低精度数据格式，它们是当前加速大型语言模型 (LLM) 推理的核心技术。这些格式主要体现了在浮点数和整数这两大类中，如何在位宽上进行精妙的取舍。</p>
<h4 id="浮点数：FP8-E4M3-和-E5M2"><a href="#浮点数：FP8-E4M3-和-E5M2" class="headerlink" title="浮点数：FP8 (E4M3 和 E5M2)"></a>浮点数：FP8 (E4M3 和 E5M2)</h4><p>FP8（8 位浮点数）格式仅使用 8 位来存储一个数值，它的关键在于这 8 位如何在指数 (E) 和尾数 (M) 之间进行分配，从而产生不同的权衡。</p>
<p>A. FP8 格式结构解析</p>
<div style="font-size: 14px;width:100%;overflow-x:auto;">
<table style="width:1800px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="left">字段</th>
<th align="left">符号位 (S)</th>
<th align="left">指数域 (E)</th>
<th align="left">尾数域 (M)</th>
<th align="left">总位数</th>
</tr>
</thead>
<tbody><tr>
<td align="left">E4M3</td>
<td align="left">1</td>
<td align="left">4</td>
<td align="left">3</td>
<td align="left">8</td>
</tr>
<tr>
<td align="left">E5M2</td>
<td align="left">1</td>
<td align="left">5</td>
<td align="left">2</td>
<td align="left">8</td>
</tr>
<tr>
<td align="left"></tr></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
</table>
</div>

<p>B. E4M3 (4 指数位, 3 尾数位)</p>
<ul>
<li>特性： 动态范围优先。<ul>
<li>指数 (E&#x3D;4): 4 位指数，动态范围比 FP16 (5 位指数) 略小，但比 E5M2 小。</li>
<li>尾数 (M&#x3D;3): 只有 3 位尾数，精度非常低，舍入误差较大。</li>
</ul>
</li>
<li>适用场景：<ul>
<li>适用于模型权重。权重相对静态，对精度要求略低，但需要一定的动态范围。</li>
<li>主要用于Transformer 模型的激活值，尤其是需要更大动态范围来处理激活值张量中可能出现的极端值（Outliers）时。</li>
</ul>
</li>
</ul>
<p>C. E5M2 (5 指数位, 2 尾数位)</p>
<ul>
<li>特性： 超高动态范围优先。<ul>
<li>指数 (E&#x3D;5): 5 位指数，与 FP16 相同。这意味着它拥有与 FP16 一样的动态范围，能表示的数值范围极大（不易溢出）。</li>
<li>尾数 (M&#x3D;2): 只有 2 位尾数，精度极低，是所有常用浮点格式中精度最低的之一。</li>
</ul>
</li>
<li>适用场景：<ul>
<li>适用于模型权重。由于其超大的动态范围，它可以更好地适应权重分布中可能出现的极端值，从而减少量化误差。</li>
<li>常用于激活值，因为激活值在计算过程中波动较大，需要高动态范围来防止数值溢出。</li>
</ul>
</li>
</ul>
<p>总结： 在深度学习中，E4M3 和 E5M2 通常配合使用，一个用于权重，一个用于激活值，以在 8 位的限制下平衡动态范围 (E) 和精度 (M)。</p>
<h4 id="整数量化：INT8-和-INT4"><a href="#整数量化：INT8-和-INT4" class="headerlink" title="整数量化：INT8 和 INT4"></a>整数量化：INT8 和 INT4</h4><p>整数格式（INT8&#x2F;INT4）通过量化 (Quantization) 将浮点数（如 FP16&#x2F;FP32）线性映射到整数值，实现极高的内存压缩和计算效率。</p>
<p>A. INT8 (W8A8) 精度</p>
<ul>
<li>含义： 8 位整数 (INT8)，是业界标准的量化格式。</li>
<li>W8A8 结构：<ul>
<li>W8 (Weight 8-bit): 模型权重被量化为 INT8。</li>
<li>A8 (Activation 8-bit): 模型激活值和计算也使用 INT8 格式。</li>
</ul>
</li>
<li>如何实现量化？<ul>
<li>量化过程需要确定一个比例因子 (Scale) 和一个零点 (Zero-point)，将原始浮点数范围 $[V_{\min}, V_{\max}]$ 映射到 INT8 范围 $[-128, 127]$。</li>
</ul>
</li>
<li>优势：<ul>
<li>极致的计算效率： 许多现代硬件（CPU&#x2F;GPU）都有高度优化的 INT8 矩阵乘法单元，能实现最高的每秒操作数 (TOPS)。</li>
<li>内存减半： 相较于 FP16 (2 字节)，INT8 (1 字节) 内存占用减半。</li>
</ul>
</li>
<li>挑战：<ul>
<li>精度损失： INT8 只有 256 个可能的离散值，量化误差比浮点格式更大，因此需要复杂的量化算法来保持模型精度。</li>
</ul>
</li>
</ul>
<p>B. INT4 (W4A16) 精度</p>
<ul>
<li>含义： 4 位整数 (INT4)，专用于大型 LLM 的极限内存压缩。</li>
<li>W4A16 结构：<ul>
<li>W4 (Weight 4-bit): 模型权重被量化为 INT4。</li>
<li>A16 (Activation 16-bit): 模型激活值和计算仍使用 FP16 格式（或 BF16）。</li>
</ul>
</li>
<li>优势：<ul>
<li>极致的内存压缩： 相比 FP16，模型权重体积减少 4 倍（从 2 字节降到 0.5 字节），对于 GigaByte 级别的 LLM 至关重要。</li>
<li>高性能计算： 虽然激活值仍是 FP16，但计算过程通过反量化或特殊硬件支持，将 INT4 权重与 FP16 激活值结合，实现高效运算。</li>
</ul>
</li>
<li>挑战：<ul>
<li>精度保持难度高： 4 位只有 16 个可能的离散值，量化精度损失风险最大。因此，必须使用像 AWQ (Activation-aware Weight Quantization) 这样的先进算法来精心挑选需要量化的权重，以保持模型性能。</li>
</ul>
</li>
</ul>
<p>📊 总结对比表</p>
<div style="font-size: 14px;width:100%;overflow-x:auto;">
<table style="width:1000px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="center">格式</th>
<th align="center">类型</th>
<th align="left">位宽 (W&#x2F;A)</th>
<th align="left">内存 (per value)</th>
<th align="left">主要优势</th>
<th align="left">主要用途</th>
</tr>
</thead>
<tbody><tr>
<td align="center">FP8</td>
<td align="center">E4M3</td>
<td align="left">浮点	8 (W8&#x2F;A8)</td>
<td align="left">1 Byte</td>
<td align="left">动态范围，用于浮点计算</td>
<td align="left">权重或激活值</td>
</tr>
<tr>
<td align="center">FP8</td>
<td align="center">E5M2</td>
<td align="left">浮点	8 (W8&#x2F;A8)</td>
<td align="left">1 Byte</td>
<td align="left">超高动态范围，极少溢出</td>
<td align="left">权重或激活值</td>
</tr>
<tr>
<td align="center">INT8</td>
<td align="center">W8A8</td>
<td align="left">整数	8 (W8&#x2F;A8)</td>
<td align="left">1 Byte</td>
<td align="left">极高计算吞吐量 (TOPS)</td>
<td align="left">图像识别等非 LLM 场景</td>
</tr>
<tr>
<td align="center"></tr></td>
<td align="center"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
</table>
</div>

<p>这些低精度格式的选择和配置，是 LLM 部署工程师们在硬件特性、内存预算和模型精度之间进行复杂权衡的结果。</p>
<h1 id="量化算法"><a href="#量化算法" class="headerlink" title="量化算法"></a>量化算法</h1><h2 id="AWQ-算法-Activation-aware-Weight-Quantization"><a href="#AWQ-算法-Activation-aware-Weight-Quantization" class="headerlink" title="AWQ 算法 (Activation-aware Weight Quantization)"></a>AWQ 算法 (Activation-aware Weight Quantization)</h2><p>AWQ (Activation-aware Weight Quantization) 算法是专门针对 W4A16（权重 INT4，激活值 FP16） 这种超低精度量化配置设计的，其核心目标是在极度压缩模型权重的同时，最大限度地保护模型的精度。</p>
<p>AWQ 提升性能的关键不在于计算速度本身（计算速度主要由 INT4 格式和硬件决定），而在于它能让模型在 INT4 下保持足够高的精度，从而使得这种高效的低精度部署在实际应用中可行。</p>
<ul>
<li>含义： 是一种感知激活值的权重（量化）算法。</li>
<li>作用： 在将权重从高精度（如 FP16）量化到超低精度（如 INT4）时，AWQ 算法会考虑激活值的重要性。</li>
<li>原理： 它不是平均地量化所有权重，而是跳过那些对最终输出影响最重要的权重（即与较大的激活值相乘的权重），确保它们能保持较高的精度。这最小化了量化对模型推理精度的负面影响，使得模型在 INT4 下仍能保持接近 FP16 的性能。</li>
</ul>
<p>核心洞察：激活值的重要性</p>
<p>传统的量化方法（如 PTQ，Post-Training Quantization）通常只关注权重本身的分布，试图将它们均匀地映射到 INT4 范围。</p>
<div style="
  border: 1px solid #ddd;        /* 边框样式 */
  padding: 10px;                 /* 内边距 */
  background-color: #f3ba2f;     /* 背景颜色 - 可修改 */
  color: #4e4e4e;                /* 文字颜色 - 可修改 */
  border-radius: 5px;            /* 圆角半径 */
  margin: 10px 0;                /* 外边距 */
">
AWQ 的核心洞察是：量化误差的影响并非均匀分布，而是取决于它与激活值的乘积。
</div>

<p>在一个矩阵乘法 Y&#x3D;W⋅X 中：</p>
<ul>
<li>W 是权重矩阵 (Weight)。</li>
<li>X 是激活值矩阵 (Activation)。</li>
</ul>
<p>如果一个权重 $w_i$ 虽然很大，但与之相乘的激活值 $x_i$ 很小，那么 $w_i$ 的量化误差对最终结果 $Y$ 的影响很小。</p>
<p>相反，如果一个权重 $w_j$ 无论大小，但与之相乘的激活值 $x_j$ 很大（即 $x_j$ 是一个“重要”的激活通道），那么 $w_j$ 的微小量化误差经过放大后，会对最终结果造成巨大的精度损失。</p>
<h3 id="AWQ-的机制：保护重要的权重"><a href="#AWQ-的机制：保护重要的权重" class="headerlink" title="AWQ 的机制：保护重要的权重"></a>AWQ 的机制：保护重要的权重</h3><p>基于上述洞察，AWQ 算法执行了一个“敏感性分析”步骤，来决定哪些权重在量化过程中需要被“保护”起来。</p>
<p><strong>步骤一：敏感性评分 (Salience Score)</strong></p>
<p>AWQ 首先计算每个输出通道的权重敏感性评分。这个评分基于激活值的统计信息（例如，激活值的最大绝对值）。</p>
<p>$$Score_i&#x3D;max(∣X_{channel,i}∣)$$</p>
<p>$$Score_i$$ 代表了第 $$i$$ 个通道的激活值的重要性。</p>
<p><strong>步骤二：权重缩放（保护高敏感通道）</strong></p>
<p>在将权重 W 量化到 INT4 之前，AWQ 会对权重应用一个通道级的缩放因子 (Scaling Factor s)。<br>AWQ 的目标是给那些具有高敏感性评分（即与大激活值相乘）的权重施加更大的缩放因子 s。<br>新的权重&#x3D;W&#x2F;s</p>
<p>施加缩放因子后，这些权重在量化到 INT4 范围时：</p>
<ol>
<li>它们在量化后的 相对误差会更小。</li>
<li>它们有效地被“隔离”了量化对激活值的影响，使得它们能够更精确地表示。</li>
</ol>
<p><strong>步骤三：四舍五入到 INT4</strong></p>
<p>最终，缩放后的权重被量化到 INT4。在推理时，这些 INT4 权重会通过反量化和 FP16 激活值进行计算。</p>
<h3 id="AWQ-如何专门提升-INT4-W4A16-的性能？"><a href="#AWQ-如何专门提升-INT4-W4A16-的性能？" class="headerlink" title="AWQ 如何专门提升 INT4 (W4A16) 的性能？"></a>AWQ 如何专门提升 INT4 (W4A16) 的性能？</h3><p>AWQ 的设计与 W4A16 配置完美契合，体现在两个方面：</p>
<p><strong>A. 专门解决 INT4 的极限精度问题</strong></p>
<p>INT4 (4 位) 只有 16 个离散值，量化是极其粗糙的。任何微小的量化误差都可能被放大，导致模型性能崩溃。AWQ 通过有针对性地保护最重要的 1% 权重，成功地避免了这种精度崩溃，使得 LLM 在 4 位下依然能保持接近 FP16 的表现。</p>
<p><strong>B. 充分利用 W4A16 的计算结构</strong></p>
<p>在 W4A16 中，激活值保持在 FP16，这为 AWQ 提供了便利。AWQ 可以利用高精度的 FP16 激活值来精确计算敏感性评分，指导权重缩放，从而得到一个对精度最友好的 INT4 权重集。如果激活值也是低精度（如 W8A8），这种精确的敏感性分析将难以进行。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>AWQ 算法通过引入激活值感知的量化策略，确保了最关键的权重能够保持最高的相对精度。这使得极度节省内存的 INT4 (W4A16) 量化配置在 LLM 上具有实用性，从而间接实现了“在可接受精度损失下，最高效的推理性能”。</p>
<h2 id="GPTQ-GPT-Quantization"><a href="#GPTQ-GPT-Quantization" class="headerlink" title="GPTQ (GPT Quantization)"></a>GPTQ (GPT Quantization)</h2><p>GPTQ 是一种离线（Post-Training）量化方法，它通过对模型权重进行逐层优化，以最小化量化带来的精度损失。<br>核心思想：逐层最小化误差</p>
<p>GPTQ 的核心是利用一种称为 Optimal Brain Damage (OBD) 或 二阶信息 的概念来决定如何量化权重。</p>
<ul>
<li>目标： 对于模型中的每一层，找到最佳的 INT4 权重，使得量化后的输出尽可能接近原始 FP16 模型的输出。</li>
<li>方法： 它不是简单地四舍五入，而是使用一种最小二乘法（Least Squares）的优化过程。在量化每一层时，它会使用一小部分校准数据（calibration data，通常几百个样本）通过最小化均方误差 (MSE) 来确定最优的量化值。</li>
<li>操作： 逐列、逐层进行优化，并使用 Hessian 矩阵的逆（或近似）来计算量化误差对其他未量化权重的影响，并进行补偿。</li>
</ul>
<p>优势与劣势</p>
<div style="font-size: 14px;width:100%;overflow-x:auto;">
<table style="width:1400px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="left">类别</th>
<th align="left">优势</th>
<th align="left">劣势</th>
</tr>
</thead>
<tbody><tr>
<td align="left">优势</td>
<td align="left">* 精度高： 通常是所有离线 4-bit 量化方法中精度表现最好的之一。* 无需训练： 属于离线量化，无需反向传播或大量训练数据。</td>
<td align="left">* 量化速度慢： 量化过程需要进行复杂的二阶优化，量化时间较长。* 占用内存高： 量化过程中需要存储大量中间数据（如 Hessian 矩阵），对 CPU&#x2F;GPU 内存要求较高。</td>
</tr>
<tr>
<td align="left"></tr></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
</table>
</div>

<h2 id="SmoothQuant"><a href="#SmoothQuant" class="headerlink" title="SmoothQuant"></a>SmoothQuant</h2><p>SmoothQuant 是一种混合量化策略，它通过平滑激活值的分布，使激活值和权重都更容易被量化到低位宽（通常是 W8A8 或 W4A8）。</p>
<p>核心思想：平衡权重和激活值的难度</p>
<p>在大型 LLM 中，激活值的分布往往比权重的分布更难处理，因为激活值中经常出现“离群值”（Outliers，少数几个极大的值），这使得 INT8&#x2F;INT4 量化难以准确覆盖整个动态范围。</p>
<div style="
  border: 1px solid #ddd;        /* 边框样式 */
  padding: 10px;                 /* 内边距 */
  background-color: #f3ba2f;     /* 背景颜色 - 可修改 */
  color: #4e4e4e;                /* 文字颜色 - 可修改 */
  border-radius: 5px;            /* 圆角半径 */
  margin: 10px 0;                /* 外边距 */
">
SmoothQuant 的策略是：将激活值中的量化难度“转移”到权重上。
</div>

<ul>
<li>平滑操作： 通过引入一个 平滑因子 s，将激活值 X 的量化难度转移给权重 W：</li>
<li>W′&#x3D;W⋅s和X′&#x3D;X&#x2F;s</li>
<li>结果： 经过 s 因子操作后：<ol>
<li>新的激活值 X′ 的分布变得更平坦，更容易进行低位宽（如 INT8）量化。</li>
<li>新的权重 W′ 的分布变得更加极端（包含更大的数值）。</li>
</ol>
</li>
<li>量化： 由于权重通常是在离线量化中处理一次，可以容忍更高的量化难度。因此，平滑后的激活值 X′使用 INT8&#x2F;INT4 量化，平滑后的权重 W′ 也使用 INT8&#x2F;INT4 量化。</li>
</ul>
<div style="font-size: 14px;width:100%;overflow-x:auto;">
<table style="width:1800px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="left">类别</th>
<th align="left">优势</th>
<th align="left">劣势</th>
</tr>
</thead>
<tbody><tr>
<td align="left">优势</td>
<td align="left">* W8A8&#x2F;W4A8 可行性： 解决了激活值难以量化的问题，使得全 INT8 (W8A8) 或 W4A8 等配置在 LLM 上可行。* 推理速度快： 权重和激活值都是低位宽，推理计算效率极高。</td>
<td align="left">* 需要预处理： 需要在推理前对权重进行一次性平滑处理。* 对权重增加难度： 虽然解决了激活值的难度，但将难度转移给了权重，可能需要更精细的权重量化。</td>
</tr>
<tr>
<td align="left"></tr></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
</table>
</div>

<h2 id="三种量化方法的区别总结"><a href="#三种量化方法的区别总结" class="headerlink" title="三种量化方法的区别总结"></a>三种量化方法的区别总结</h2><div style="font-size: 14px;width:100%;overflow-x:auto;">
<table style="width:1800px;">
  <tr>
    <td>

<table>
<thead>
<tr>
<th align="left">特征</th>
<th align="left">AWQ (Activation-aware Weight Quantization)</th>
<th align="left">GPTQ (GPT Quantization)	SmoothQuant</th>
</tr>
</thead>
<tbody><tr>
<td align="left">主要目标</td>
<td align="left">W4A16 保持高精度</td>
<td align="left">W4A16 保持高精度</td>
</tr>
<tr>
<td align="left">核心机制</td>
<td align="left">保护权重：基于激活值的重要性（最大值），保护高敏感度的权重。</td>
<td align="left">最小化误差：通过二阶优化逐层计算和补偿量化误差。</td>
</tr>
<tr>
<td align="left">处理对象</td>
<td align="left">仅缩放权重。利用激活值信息指导权重缩放。</td>
<td align="left">仅优化权重。优化权重使输出误差最小。</td>
</tr>
<tr>
<td align="left">量化速度</td>
<td align="left">快。量化过程简单高效。</td>
<td align="left">慢。涉及复杂的二阶优化。</td>
</tr>
<tr>
<td align="left">适用场景</td>
<td align="left">内存受限，需要极致压缩，同时保持推理速度（W4A16）。</td>
<td align="left">需要最高精度保证，且能容忍较慢的离线量化时间。</td>
</tr>
<tr>
<td align="left"></tr></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
</table>
</div>

<p>实际应用中的选择：</p>
<ol>
<li>如果您需要最低的内存占用，并且能接受 W4A16（权重 4 位，激活值 16 位）的混合计算模式，并且追求最快的量化速度，请选择 AWQ。</li>
<li>如果您对模型的推理精度要求极高，并且可以接受较慢的离线量化时间，请选择 GPTQ。</li>
<li>如果您想让权重和激活值都实现低位宽（如 W8A8），以最大化硬件的 INT8 计算吞吐量，请选择 SmoothQuant。</li>
</ol>
<h2 id="为什么需要混合精度？（FP8-NVFP4-vs-FP16）"><a href="#为什么需要混合精度？（FP8-NVFP4-vs-FP16）" class="headerlink" title="为什么需要混合精度？（FP8&#x2F;NVFP4 vs. FP16）"></a>为什么需要混合精度？（FP8&#x2F;NVFP4 vs. FP16）</h2><p>在 LLM 推理中，该 SDK 采用了一种混合精度策略：<br>权重和 GEMM 操作采用 FP8 或 NVFP4 格式，而 LayerNorm、KV 缓存、LM 头部和注意力层则保持 FP16 格式。<br>这样做是为了在性能和精度之间取得最佳平衡：</p>
<h3 id="采用低精度（FP8-NVFP4）的部分："><a href="#采用低精度（FP8-NVFP4）的部分：" class="headerlink" title="采用低精度（FP8&#x2F;NVFP4）的部分："></a>采用低精度（FP8&#x2F;NVFP4）的部分：</h3><ul>
<li>权重 (Weights): 模型权重是模型中数量最大的部分，压缩它们可以显著节省内存。</li>
<li>GEMM 操作 (General Matrix Multiply): 这是 LLM 推理过程中计算量最大、耗时最久的部分（主要是矩阵乘法）。使用 FP8&#x2F;NVFP4 可以在支持的硬件上调用高度优化的低精度计算核，从而大幅提升推理速度（内核性能）。</li>
</ul>
<h3 id="保持高精度（FP16）的部分："><a href="#保持高精度（FP16）的部分：" class="headerlink" title="保持高精度（FP16）的部分："></a>保持高精度（FP16）的部分：</h3><ul>
<li>LayerNorm (层归一化): 这是一个稳定性要求很高的操作。如果在这里使用过低的精度，很可能会导致数值溢出或欠流，从而使模型输出迅速崩溃或失去有效性。</li>
<li>KV 缓存 (Key&#x2F;Value Cache): KV 缓存存储了中间计算结果，会重复多次被访问和使用。如果精度太低，误差会随着序列长度的增加而累积，最终影响生成文本的质量。</li>
<li>LM 头部 (Language Model Head) 和注意力层 (Attention Layers): 这些是决定最终输出结果的关键部分。保持 FP16 能够保证最终的 Logits 具有足够的数值精度，从而确保模型生成的结果是准确的。</li>
</ul>
<p>简而言之，计算量大且对精度不太敏感的部分（如 GEMM）使用低精度来加速；而对数值稳定性或最终输出质量至关重要的部分（如 LayerNorm、KV 缓存）则保持 FP16 来保证准确性。</p>


  </article>
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <div class="busuanzi center">
    <span id="busuanzi_container_page_pv">
      本文阅读量：<span id="busuanzi_value_page_pv"></span> 次
    </span>
    <span id="busuanzi_container_site_pv">
      本站总访问量：<span id="busuanzi_value_site_pv"></span> 次
    </span>
    <span id="busuanzi_container_site_uv">
      本站访客数：<span id="busuanzi_value_site_uv"></span> 人
    </span>
  </div>

<!-- 
    
        <div id="disqus_thread"></div>
        <script>

        var disqus_config = function () {
        this.page.url = 'https://www.liangshuangsupercool.com/2025/04/09/GPUseries/index.html';
        this.page.identifier = ;
        };

        (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');

        s.src = '//supercool-2.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
        })();
        </script>
    
 -->


    <div id="disqus_thread"></div>
    <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
        /*
        var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        */
        (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://supercool-2.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


 <!-- 
    <div id="disqus_thread"></div>
    <script>
        var disqus_config = function () {
            this.page.url = 'https://www.liangshuangsupercool.com/2025/04/09/GPUseries/';
            this.page.identifier = '2025/04/09/GPUseries/';
            this.page.title = 'GPU性能比较';
        };
        
        (function() {
            var d = document, s = d.createElement('script');
            s.src = 'https://supercool-2.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>请启用JavaScript查看<a target="_blank" rel="noopener" href="https://disqus.com/?ref_noscript">Disqus评论</a></noscript>
 -->


    </div>
  </div>
  <footer class="page-footer"><div class="clearfix">
</div>
<div class="right-foot">
    <div class="firstrow">
        <a href="#top" target="_self">
        <svg class="i-caret-right" viewBox="0 0 32 32" width="24" height="24" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3">
            <path d="M10 30 L26 16 10 2 Z"></path>
        </svg>
        </a>
        liangshuang 1999-2025
    </div>
    <div class="secondrow">
        <a target="_blank" rel="noopener" href="https://github.com/supercooledith">
        supercool
        </a>
    </div>
</div>
<div class="clearfix">
</div>
</footer>
  <script type="text/javascript">

// disqus scripts

var disqus_shortname = 'supercool-2';
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('body')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());


// dropdown scripts
function $(s) {
    return document.querySelector(s)
  }

  function $$(s) {
    return document.querySelectorAll(s)
  }
  var dropDownElArr = $$('.dropdown')
  for (var i = 0; i < dropDownElArr.length; i += 1) {
    dropDownElArr[i].addEventListener('click', function(event) {
      var content = this.querySelector('.dropdown-content')
      event.stopPropagation()
      if (content.className.indexOf('open') < 0) {
        content.classList.add('open')
      } else {
        content.classList.remove('open')
      }
    })
  }
  document.body.addEventListener('click', function() {
    var dropDownContentElArr = $$('.dropdown-content')
    for (var i = 0; i < dropDownContentElArr.length; i += 1) {
      dropDownContentElArr[i].classList.remove('open')
    }
  })

  // smooth scroll
  $$('a[href^="#"]').forEach(e=>{
    e.addEventListener("click", function(e) {
      e.preventDefault();
      var t = this.getAttribute("href").substr(1);
      $(`[id='${decodeURIComponent(t)}']`).scrollIntoView({
        behavior: "smooth"
      }),
      t === "top" ? history.replaceState(null, null, " ") : history.pushState(null, null, `#${t}`)
    })
  })

// search
var searchEl = document.querySelector("#local-search-input")
if (searchEl) searchEl.addEventListener('keydown', function(e) {
  if (e.key === 'Enter') {
    var s = e.target.value
    window.open('//www.google.com/search?q=site:https://www.liangshuangsupercool.com%20' + s, '_blank').focus()
  }
})

</script>

</body>
</html>
